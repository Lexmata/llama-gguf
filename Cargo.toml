[package]
name = "llama-gguf"
version = "0.7.3"
edition = "2024"
license = "MIT OR Apache-2.0"
authors = ["Lexmata LLC <jquinn@lexmata.ai>"]
description = "A high-performance Rust implementation of llama.cpp - LLM inference engine with full GGUF support"
repository = "https://github.com/Lexmata/llama-gguf"
keywords = ["llm", "inference", "gguf", "llama", "ai"]
categories = ["science", "algorithms"]

[lib]
name = "llama_gguf"
path = "src/lib.rs"

[[bin]]
name = "llama-gguf"
path = "src/main.rs"
required-features = ["cli"]

[features]
default = ["cpu", "huggingface", "cli", "client", "onnx"]
cli = ["dep:clap", "dep:clap_mangen"]
client = ["dep:reqwest"]
cpu = []
cuda = ["dep:cudarc"]
vulkan = ["dep:ash", "dep:gpu-allocator"]
vulkan-shaders = []  # Enable build-time shader compilation (requires glslc)
metal = ["dep:metal", "dep:objc"]
dx12 = ["dep:windows"]
server = ["dep:axum", "dep:tokio", "dep:tower-http", "dep:futures"]
rag = ["dep:tokio-postgres", "dep:pgvector", "dep:deadpool-postgres", "dep:tokio", "dep:url", "dep:glob"]
huggingface = ["dep:reqwest", "dep:indicatif", "dep:directories"]
onnx = ["dep:prost"]
distributed = ["dep:tonic", "dep:tokio", "dep:prost", "dep:futures"]

[dependencies]
memmap2 = "0.9"
bytemuck = { version = "1.14", features = ["derive"] }
half = { version = "2.3", features = ["bytemuck"] }
rayon = "1.8"
thiserror = "1.0"
tracing = "0.1"
clap = { version = "4.4", features = ["derive", "env"], optional = true }
clap_mangen = { version = "0.2", optional = true }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
rand = "0.8"
reqwest = { version = "0.12", default-features = false, features = ["blocking", "json", "rustls-tls"], optional = true }
indicatif = { version = "0.17", optional = true }
directories = { version = "5.0", optional = true }

cudarc = { version = "0.12", optional = true, features = ["cuda-12050"] }
ash = { version = "0.38", optional = true, features = ["loaded", "debug"] }
gpu-allocator = { version = "0.28", optional = true, default-features = false, features = ["vulkan", "std"] }
metal = { version = "0.28", optional = true }
objc = { version = "0.2", optional = true }
windows = { version = "0.58", optional = true, features = [
    "Win32_Foundation",
    "Win32_Graphics_Direct3D",
    "Win32_Graphics_Direct3D12",
    "Win32_Graphics_Dxgi",
    "Win32_Graphics_Dxgi_Common",
    "Win32_Security",
    "Win32_System_Threading",
] }

axum = { version = "0.7", optional = true }
tokio = { version = "1.35", features = ["full"], optional = true }
tower-http = { version = "0.5", features = ["cors"], optional = true }
futures = { version = "0.3", optional = true }

# RAG / pgvector support
tokio-postgres = { version = "0.7", optional = true, features = ["with-serde_json-1"] }
pgvector = { version = "0.4", features = ["postgres"], optional = true }
deadpool-postgres = { version = "0.14", optional = true }
url = { version = "2.5", optional = true }
glob = { version = "0.3", optional = true }
toml = "0.8"

# ONNX protobuf support
prost = { version = "0.13", optional = true }

# Distributed inference (gRPC)
tonic = { version = "0.12", optional = true }

[build-dependencies]
prost-build = "0.13"
tonic-build = "0.12"

[dev-dependencies]
criterion = { version = "0.8", features = ["html_reports"] }
tempfile = "3.10"

[profile.release]
lto = true
codegen-units = 1

[[bench]]
name = "quantization"
harness = false
